{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow LongFormer NER Baseline\n",
    "\n",
    "This notebook is a TensorFlow starter notebook for Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Currently this notebook uses\n",
    "* backbone LongFormer\n",
    "* NER formulation\n",
    "* one fold\n",
    "\n",
    "With simple changes, we can convert this notebook into Question Answer formulation and we can try different backbones. Furthermore this notebook is one fold. It trains with 90% data and validates on 10% data. We can convert this notebook to K-fold or train with 100% data for boost in LB.\n",
    "\n",
    "The transformer model LongFormer is explained [here][1]. It is similar to Roberta but can accept inputs as wide as 4096 tokens! In this notebook we feed the transformer with 1024 wide tokens. HuggingFace user AllenAI uploaded pretrained weights for us [here][2]\n",
    "\n",
    "[1]: https://huggingface.co/docs/transformers/model_doc/longformer\n",
    "[2]: https://huggingface.co/allenai/longformer-base-4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:51:42.294599Z",
     "iopub.status.busy": "2021-12-22T04:51:42.294417Z",
     "iopub.status.idle": "2021-12-22T04:52:17.681470Z",
     "shell.execute_reply": "2021-12-22T04:52:17.680824Z",
     "shell.execute_reply.started": "2021-12-22T04:51:42.294542Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/envs/saturn/lib/python3.9/site-packages (1.5.12)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from kaggle) (1.26.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/saturn/lib/python3.9/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/saturn/lib/python3.9/site-packages (from kaggle) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/saturn/lib/python3.9/site-packages (from kaggle) (4.62.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/envs/saturn/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/envs/saturn/lib/python3.9/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->kaggle) (3.3)\n",
      "feedback-prize-2021.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "tf-longformer-v12.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "import zipfile\n",
    "\n",
    "!kaggle competitions download -c feedback-prize-2021 -p ../input/feedback-prize-2021 \n",
    "!kaggle datasets download  -d cdeotte/tf-longformer-v12 -p ../input/tf-longformer-v12 \n",
    "\n",
    "with zipfile.ZipFile('../input/feedback-prize-2021/feedback-prize-2021.zip','r') as myzip:\n",
    "    myzip.extractall('../input/feedback-prize-2021')\n",
    "    \n",
    "with zipfile.ZipFile('../input/tf-longformer-v12/tf-longformer-v12.zip','r') as myzip:\n",
    "    myzip.extractall('../input/tf-longformer-v12')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "This notebook can either train a new model or load a previously trained model (made from previous notebook version). Furthermore, this notebook can either create new NER tokens or load existing tokens (made from previous notebook version). In this notebook version, we will load model and load NER tokens. \n",
    "\n",
    "Also this notebook can load huggingface stuff (like tokenizers) from a Kaggle dataset, or download it from internet. (If it downloads from internet, you can then put it in a Kaggle dataset, so next time you can turn internet off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:17.682737Z",
     "iopub.status.busy": "2021-12-22T04:52:17.682496Z",
     "iopub.status.idle": "2021-12-22T04:52:17.686709Z",
     "shell.execute_reply": "2021-12-22T04:52:17.686278Z",
     "shell.execute_reply.started": "2021-12-22T04:52:17.682715Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# DECLARE HOW MANY GPUS YOU WISH TO USE. \n",
    "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #0,1,2,3 for four gpu\n",
    "\n",
    "# VERSION FOR SAVING MODEL WEIGHTS\n",
    "VER=12\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = '../input/tf-longformer-v12'\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "LOAD_MODEL_FROM = '../input/tf-longformer-v12'\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK \n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE \n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "DOWNLOADED_MODEL_PATH = '../input/tf-longformer-v12'\n",
    "\n",
    "if DOWNLOADED_MODEL_PATH is None:\n",
    "    DOWNLOADED_MODEL_PATH = 'model'    \n",
    "MODEL_NAME = 'allenai/longformer-base-4096'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Submit TensorFlow Without Internet\n",
    "Many people ask me, how do I submit TensorFlow models without internet? With HuggingFace Transformer, it's easy. Just download the following 3 things (1) model weights, (2) tokenizer files, (3) config file, and upload them to a Kaggle dataset. Below shows code how to get the files from HuggingFace for AllenAI's model `longformer-base`. But this same code can download any transformer, like for example `roberta-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:17.688241Z",
     "iopub.status.busy": "2021-12-22T04:52:17.688034Z",
     "iopub.status.idle": "2021-12-22T04:52:17.701166Z",
     "shell.execute_reply": "2021-12-22T04:52:17.700752Z",
     "shell.execute_reply.started": "2021-12-22T04:52:17.688223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config.save_pretrained('model')\n",
    "\n",
    "    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above saves the files\n",
    "* TOKENIZER FILES - merges.txt, tokenizer_config.json, special_tokens_map.json, tokenizer.json, vocab.json\n",
    "* CONFIG FILE - config.json\n",
    "* MODEL WEIGHT FILE - tf_model.h5\n",
    "\n",
    "Then just upload all these files to a Kaggle dataset, like what I did [here][1]. Then you load them into your notebook like the notebook you are reading. And we can turn internet off!\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tf-longformer-v12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:17.701966Z",
     "iopub.status.busy": "2021-12-22T04:52:17.701774Z",
     "iopub.status.idle": "2021-12-22T04:52:25.365951Z",
     "shell.execute_reply": "2021-12-22T04:52:25.365403Z",
     "shell.execute_reply.started": "2021-12-22T04:52:17.701949Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/saturn/lib/python3.9/site-packages (0.1.96)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.62 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow==2.62\u001b[0m\n",
      "Requirement already satisfied: transformers==4.12.5 in /opt/conda/envs/saturn/lib/python3.9/site-packages (4.12.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (0.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (1.19.5)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers==4.12.5) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.12.5) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers==4.12.5) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers==4.12.5) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: six in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sacremoses->transformers==4.12.5) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sacremoses->transformers==4.12.5) (8.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 04:52:24.367160: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install tensorflow==2.62\n",
    "!pip install transformers==4.12.5\n",
    "\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:25.367009Z",
     "iopub.status.busy": "2021-12-22T04:52:25.366775Z",
     "iopub.status.idle": "2021-12-22T04:52:25.371020Z",
     "shell.execute_reply": "2021-12-22T04:52:25.370572Z",
     "shell.execute_reply.started": "2021-12-22T04:52:25.366989Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single strategy\n"
     ]
    }
   ],
   "source": [
    "# USE MULTIPLE GPUS\n",
    "if os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('single strategy')\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('multiple strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:25.371909Z",
     "iopub.status.busy": "2021-12-22T04:52:25.371724Z",
     "iopub.status.idle": "2021-12-22T04:52:25.375052Z",
     "shell.execute_reply": "2021-12-22T04:52:25.374612Z",
     "shell.execute_reply.started": "2021-12-22T04:52:25.371891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:25.375873Z",
     "iopub.status.busy": "2021-12-22T04:52:25.375695Z",
     "iopub.status.idle": "2021-12-22T04:52:26.103287Z",
     "shell.execute_reply": "2021-12-22T04:52:26.102835Z",
     "shell.execute_reply.started": "2021-12-22T04:52:25.375856Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144293, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "print( train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:26.104295Z",
     "iopub.status.busy": "2021-12-22T04:52:26.104076Z",
     "iopub.status.idle": "2021-12-22T04:52:26.116038Z",
     "shell.execute_reply": "2021-12-22T04:52:26.115613Z",
     "shell.execute_reply.started": "2021-12-22T04:52:26.104276Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train labels are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
       "       'Counterclaim', 'Rebuttal'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The train labels are:')\n",
    "train.discourse_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:26.116933Z",
     "iopub.status.busy": "2021-12-22T04:52:26.116748Z",
     "iopub.status.idle": "2021-12-22T04:52:26.128236Z",
     "shell.execute_reply": "2021-12-22T04:52:26.127773Z",
     "shell.execute_reply.started": "2021-12-22T04:52:26.116916Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts.\n"
     ]
    }
   ],
   "source": [
    "IDS = train.id.unique()\n",
    "print('There are',len(IDS),'train texts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Train\n",
    "The following code converts Kaggle's train dataset into a NER token array that we can use to train a NER transformer. I have made it very clear which targets belong to which class. This allows us to very easily convert this code to `Question Answer formulation` if we want. Just change the 14 NER arrays to be 14 arrays of `start position` and `end position` for each of the 7 classes. (You will need to think creatively what to do if a single text has multiple of one class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:26.129232Z",
     "iopub.status.busy": "2021-12-22T04:52:26.128923Z",
     "iopub.status.idle": "2021-12-22T04:52:26.429176Z",
     "shell.execute_reply": "2021-12-22T04:52:26.428649Z",
     "shell.execute_reply.started": "2021-12-22T04:52:26.129214Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# THE TOKENS AND ATTENTION ARRAYS\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\n",
    "train_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "train_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "\n",
    "# THE 14 CLASSES FOR NER\n",
    "lead_b = np.zeros((len(IDS),MAX_LEN))\n",
    "lead_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "position_b = np.zeros((len(IDS),MAX_LEN))\n",
    "position_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "evidence_b = np.zeros((len(IDS),MAX_LEN))\n",
    "evidence_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "claim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "claim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "conclusion_b = np.zeros((len(IDS),MAX_LEN))\n",
    "conclusion_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "counterclaim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "counterclaim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "rebuttal_b = np.zeros((len(IDS),MAX_LEN))\n",
    "rebuttal_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "# HELPER VARIABLES\n",
    "train_lens = []\n",
    "targets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\n",
    "targets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\n",
    "target_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n",
    "             'Counterclaim':5, 'Rebuttal':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:26.430210Z",
     "iopub.status.busy": "2021-12-22T04:52:26.429987Z",
     "iopub.status.idle": "2021-12-22T04:52:28.262769Z",
     "shell.execute_reply": "2021-12-22T04:52:28.262248Z",
     "shell.execute_reply.started": "2021-12-22T04:52:26.430191Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\n",
    "assert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 )\n",
    "\n",
    "# FOR LOOP THROUGH EACH TRAIN TEXT\n",
    "for id_num in range(len(IDS)):\n",
    "    if LOAD_TOKENS_FROM: break\n",
    "    if id_num%100==0: print(id_num,', ',end='')\n",
    "        \n",
    "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
    "    n = IDS[id_num]\n",
    "    name = f'../input/feedback-prize-2021/train/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    train_lens.append( len(txt.split()))\n",
    "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "    train_tokens[id_num,] = tokens['input_ids']\n",
    "    train_attention[id_num,] = tokens['attention_mask']\n",
    "    \n",
    "    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n",
    "    offsets = tokens['offset_mapping']\n",
    "    offset_index = 0\n",
    "    df = train.loc[train.id==n]\n",
    "    for index,row in df.iterrows():\n",
    "        a = row.discourse_start\n",
    "        b = row.discourse_end\n",
    "        if offset_index>len(offsets)-1:\n",
    "            break\n",
    "        c = offsets[offset_index][0]\n",
    "        d = offsets[offset_index][1]\n",
    "        beginning = True\n",
    "        while b>c:\n",
    "            if (c>=a)&(b>=d):\n",
    "                k = target_map[row.discourse_type]\n",
    "                if beginning:\n",
    "                    targets_b[k][id_num][offset_index] = 1\n",
    "                    beginning = False\n",
    "                else:\n",
    "                    targets_i[k][id_num][offset_index] = 1\n",
    "            offset_index += 1\n",
    "            if offset_index>len(offsets)-1:\n",
    "                break\n",
    "            c = offsets[offset_index][0]\n",
    "            d = offsets[offset_index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:28.265071Z",
     "iopub.status.busy": "2021-12-22T04:52:28.264837Z",
     "iopub.status.idle": "2021-12-22T04:52:28.267879Z",
     "shell.execute_reply": "2021-12-22T04:52:28.267435Z",
     "shell.execute_reply.started": "2021-12-22T04:52:28.265051Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    plt.hist(train_lens,bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:28.268791Z",
     "iopub.status.busy": "2021-12-22T04:52:28.268506Z",
     "iopub.status.idle": "2021-12-22T04:52:28.272335Z",
     "shell.execute_reply": "2021-12-22T04:52:28.271900Z",
     "shell.execute_reply.started": "2021-12-22T04:52:28.268773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n",
    "    for k in range(7):\n",
    "        targets[:,:,2*k] = targets_b[k]\n",
    "        targets[:,:,2*k+1] = targets_i[k]\n",
    "    targets[:,:,14] = 1-np.max(targets,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:28.273296Z",
     "iopub.status.busy": "2021-12-22T04:52:28.273008Z",
     "iopub.status.idle": "2021-12-22T04:52:28.613265Z",
     "shell.execute_reply": "2021-12-22T04:52:28.612767Z",
     "shell.execute_reply.started": "2021-12-22T04:52:28.273279Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NER tokens\n"
     ]
    }
   ],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    np.save(f'targets_{MAX_LEN}', targets)\n",
    "    np.save(f'tokens_{MAX_LEN}', train_tokens)\n",
    "    np.save(f'attention_{MAX_LEN}', train_attention)\n",
    "    print('Saved NER tokens')\n",
    "else:\n",
    "    targets = np.load(f'{LOAD_TOKENS_FROM}/targets_{MAX_LEN}.npy')\n",
    "    train_tokens = np.load(f'{LOAD_TOKENS_FROM}/tokens_{MAX_LEN}.npy')\n",
    "    train_attention = np.load(f'{LOAD_TOKENS_FROM}/attention_{MAX_LEN}.npy')\n",
    "    print('Loaded NER tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "We will use LongFormer backbone and add our own NER head using one hidden layer of size 256 and one final layer with softmax. We use 15 classes because we have a `B` class and `I` class for each of 7 labels. And we have an additional class (called `O` class) for tokens that do not belong to one of the 14 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:28.614276Z",
     "iopub.status.busy": "2021-12-22T04:52:28.614055Z",
     "iopub.status.idle": "2021-12-22T04:52:28.622435Z",
     "shell.execute_reply": "2021-12-22T04:52:28.621975Z",
     "shell.execute_reply.started": "2021-12-22T04:52:28.614256Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, TFAutoModel\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n",
    "    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n",
    "    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH+'/tf_model.h5', config=config)\n",
    "    \n",
    "    x = backbone(tokens, attention_mask=attention)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n",
    "    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-4),\n",
    "                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n",
    "                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:28.623298Z",
     "iopub.status.busy": "2021-12-22T04:52:28.623112Z",
     "iopub.status.idle": "2021-12-22T04:52:44.954596Z",
     "shell.execute_reply": "2021-12-22T04:52:44.954082Z",
     "shell.execute_reply.started": "2021-12-22T04:52:28.623280Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 04:52:28.653049: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-22 04:52:28.654022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-12-22 04:52:28.676508: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-12-22 04:52:28.676542: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: j-ajenn-tf-longformer-ed955743d6e9487fb3db6a42ddc73f73-75cdjbjx\n",
      "2021-12-22 04:52:28.676551: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: j-ajenn-tf-longformer-ed955743d6e9487fb3db6a42ddc73f73-75cdjbjx\n",
      "2021-12-22 04:52:28.676670: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\n",
      "2021-12-22 04:52:28.676694: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\n",
      "2021-12-22 04:52:28.676702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\n",
      "2021-12-22 04:52:28.676998: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-22 04:52:28.677629: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "All model checkpoint layers were used when initializing TFLongformerModel.\n",
      "\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at ../input/tf-longformer-v12/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFLongformerMainLayer.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer object at 0x7f55f76f09a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmptpe9aa3o.py, line 120)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFLongformerMainLayer.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer object at 0x7f55f76f09a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmptpe9aa3o.py, line 120)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFLongformerEncoder.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder object at 0x7f55f768b100>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp1hi92fgr.py, line 33)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFLongformerEncoder.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder object at 0x7f55f768b100>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp1hi92fgr.py, line 33)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFLongformerSelfAttention.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention object at 0x7f55f7682fa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpgfrik89n.py, line 53)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFLongformerSelfAttention.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention object at 0x7f55f7682fa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpgfrik89n.py, line 53)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFLongformerPooler.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerPooler object at 0x7f55f768b640>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpfzuo6ceb.py, line 10)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFLongformerPooler.call of <transformers.models.longformer.modeling_tf_longformer.TFLongformerPooler object at 0x7f55f768b640>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpfzuo6ceb.py, line 10)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train or Load Model\n",
    "If you provide a path in variable `LOAD_MODEL_FROM` above, then it will load your previously trained model. Otherwise it will train now. \n",
    "\n",
    "We train 5 epochs of batch size 32 using learning rate `1e4` for the first four and `1e5` for the last epoch. I trained my model offline. If you wish to train on Kaggle's GPU, we may need to reduce the batch size. If we reduce the batch size to 8. That is 1/4 original. So we should also reduce the learning rates to `0.25e-4` and `0.25e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:44.955585Z",
     "iopub.status.busy": "2021-12-22T04:52:44.955349Z",
     "iopub.status.idle": "2021-12-22T04:52:44.962039Z",
     "shell.execute_reply": "2021-12-22T04:52:44.961585Z",
     "shell.execute_reply.started": "2021-12-22T04:52:44.955565Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 14034 , Valid size 1560\n"
     ]
    }
   ],
   "source": [
    "# TRAIN VALID SPLIT 90% 10%\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "np.random.seed(None)\n",
    "print('Train size',len(train_idx),', Valid size',len(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:44.963070Z",
     "iopub.status.busy": "2021-12-22T04:52:44.962702Z",
     "iopub.status.idle": "2021-12-22T04:52:44.965869Z",
     "shell.execute_reply": "2021-12-22T04:52:44.965442Z",
     "shell.execute_reply.started": "2021-12-22T04:52:44.963053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\n",
    "EPOCHS = 5\n",
    "LRS = [1e-4, 1e-4, 1e-4, 1e-4, 1e-5]\n",
    "def lrfn(epoch):\n",
    "    return LRS[epoch]\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:44.966774Z",
     "iopub.status.busy": "2021-12-22T04:52:44.966471Z",
     "iopub.status.idle": "2021-12-22T04:52:45.405915Z",
     "shell.execute_reply": "2021-12-22T04:52:45.405405Z",
     "shell.execute_reply.started": "2021-12-22T04:52:44.966757Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "if LOAD_MODEL_FROM:\n",
    "    model.load_weights(f'{LOAD_MODEL_FROM}/long_v{VER}.h5')\n",
    "    \n",
    "# OR TRAIN MODEL\n",
    "else:\n",
    "    model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n",
    "          y = targets[train_idx,],\n",
    "          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n",
    "                             targets[valid_idx,]),\n",
    "          callbacks = [lr_callback],\n",
    "          epochs = EPOCHS,\n",
    "          batch_size = 32,\n",
    "          verbose = 2)\n",
    "\n",
    "    # SAVE MODEL WEIGHTS\n",
    "    model.save_weights(f'long_v{VER}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model - Infer OOF\n",
    "We will now make predictions on the validation texts. Our model makes label predictions for each token, we need to convert this into a list of word indices for each label. Note that the tokens and words are not the same. A single word may be broken into multiple tokens. Therefore we need to first create a map to change token indices to word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T04:52:45.406811Z",
     "iopub.status.busy": "2021-12-22T04:52:45.406660Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 04:52:45.433576: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-12-22 04:52:45.433966: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\n",
      "2021-12-22 04:52:45.434293: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n",
      "2021-12-22 04:52:45.435091: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n",
      "2021-12-22 04:52:45.440994: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n",
      "2021-12-22 04:52:45.442254: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n",
      "2021-12-22 04:52:45.444512: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n",
      "2021-12-22 04:52:45.445395: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n",
      "2021-12-22 04:52:56.320204: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n",
      "2021-12-22 04:52:57.642938: W tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2008] No (suitable) GPUs detected, skipping auto_mixed_precision_cuda graph optimizer\n"
     ]
    }
   ],
   "source": [
    "p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n",
    "                  batch_size=16, verbose=2)\n",
    "print('OOF predictions shape:',p.shape)\n",
    "oof_preds = np.argmax(p,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
    "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n",
    "    all_predictions = []\n",
    "\n",
    "    for id_num in range(len(preds)):\n",
    "    \n",
    "        # GET ID\n",
    "        if (id_num%100==0)&(verbose): \n",
    "            print(id_num,', ',end='')\n",
    "        n = text_ids[id_num]\n",
    "    \n",
    "        # GET TOKEN POSITIONS IN CHARS\n",
    "        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'\n",
    "        txt = open(name, 'r').read()\n",
    "        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "        off = tokens['offset_mapping']\n",
    "    \n",
    "        # GET WORD POSITIONS IN CHARS\n",
    "        w = []\n",
    "        blank = True\n",
    "        for i in range(len(txt)):\n",
    "            if (txt[i]!=' ')&(txt[i]!='\\n')&(blank==True):\n",
    "                w.append(i)\n",
    "                blank=False\n",
    "            elif (txt[i]==' ')|(txt[i]=='\\n'):\n",
    "                blank=True\n",
    "        w.append(1e6)\n",
    "            \n",
    "        # MAPPING FROM TOKENS TO WORDS\n",
    "        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n",
    "        w_i = 0\n",
    "        for i in range(len(off)):\n",
    "            if off[i][1]==0: continue\n",
    "            while off[i][0]>=w[w_i+1]: w_i += 1\n",
    "            word_map[i] = int(w_i)\n",
    "        \n",
    "        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n",
    "        # KEY:\n",
    "        # 0: LEAD_B, 1: LEAD_I\n",
    "        # 2: POSITION_B, 3: POSITION_I\n",
    "        # 4: EVIDENCE_B, 5: EVIDENCE_I\n",
    "        # 6: CLAIM_B, 7: CLAIM_I\n",
    "        # 8: CONCLUSION_B, 9: CONCLUSION_I\n",
    "        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n",
    "        # 12: REBUTTAL_B, 13: REBUTTAL_I\n",
    "        # 14: NOTHING i.e. O\n",
    "        pred = preds[id_num,]/2.0\n",
    "    \n",
    "        i = 0\n",
    "        while i<MAX_LEN:\n",
    "            prediction = []\n",
    "            start = pred[i]\n",
    "            if start in [0,1,2,3,4,5,6,7]:\n",
    "                prediction.append(word_map[i])\n",
    "                i += 1\n",
    "                if i>=MAX_LEN: break\n",
    "                while pred[i]==start+0.5:\n",
    "                    if not word_map[i] in prediction:\n",
    "                        prediction.append(word_map[i])\n",
    "                    i += 1\n",
    "                    if i>=MAX_LEN: break\n",
    "            else:\n",
    "                i += 1\n",
    "            prediction = [x for x in prediction if x!=-1]\n",
    "            if len(prediction)>4:\n",
    "                all_predictions.append( (n, target_map_rev[int(start)], \n",
    "                                ' '.join([str(x) for x in prediction]) ) )\n",
    "                \n",
    "    # MAKE DATAFRAME\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df.columns = ['id','class','predictionstring']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\n",
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('The following classes are present in oof preds:')\n",
    "oof['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Validation Metric\n",
    "The following code is from Rob Mulla's excellent notebook [here][2]. Our LongFormer single fold model achieves CV score 0.617! Hooray!\n",
    "\n",
    "[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM : Rob Mulla @robikscube\n",
    "# https://www.kaggle.com/robikscube/student-writing-competition-twitch\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID DATAFRAME\n",
    "valid = train.loc[train['id'].isin(IDS[valid_idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = []\n",
    "CLASSES = oof['class'].unique()\n",
    "for c in CLASSES:\n",
    "    pred_df = oof.loc[oof['class']==c].copy()\n",
    "    gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
    "    f1 = score_feedback_comp(pred_df, gt_df)\n",
    "    print(c,f1)\n",
    "    f1s.append(f1)\n",
    "print()\n",
    "print('Overall',np.mean(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Test Data\n",
    "We will now infer the test data and create a submission. Our CV is 0.617, let's see what our LB is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TEST TEXT IDS\n",
    "files = os.listdir('../input/feedback-prize-2021/test')\n",
    "TEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "print('There are',len(TEST_IDS),'test texts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TEST TEXT TO TOKENS\n",
    "test_tokens = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n",
    "test_attention = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n",
    "\n",
    "for id_num in range(len(TEST_IDS)):\n",
    "        \n",
    "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
    "    n = TEST_IDS[id_num]\n",
    "    name = f'../input/feedback-prize-2021/test/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "    test_tokens[id_num,] = tokens['input_ids']\n",
    "    test_attention[id_num,] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFER TEST TEXTS\n",
    "p = model.predict([test_tokens, test_attention], \n",
    "                  batch_size=16, verbose=2)\n",
    "print('Test predictions shape:',p.shape)\n",
    "test_preds = np.argmax(p,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TEST PREDICIONS\n",
    "sub = get_preds( dataset='test', verbose=False, text_ids=TEST_IDS, preds=test_preds )\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WRITE SUBMISSION CSV\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
